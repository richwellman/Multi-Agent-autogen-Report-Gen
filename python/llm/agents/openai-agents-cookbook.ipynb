{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/richwellman/Multi-Agent-autogen-Report-Gen/blob/main/python/llm/agents/openai-agents-cookbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUknhuHKyc-E"
      },
      "source": [
        "\n",
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "    <img alt=\"arize logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"300\"/>\n",
        "        <br>\n",
        "        <a href=\"https://docs.arize.com/arize/\">Docs</a>\n",
        "        |\n",
        "        <a href=\"https://github.com/Arize-ai/client_python\">GitHub</a>\n",
        "        |\n",
        "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg\">Community</a>\n",
        "    </p>\n",
        "</center>\n",
        "\n",
        "# <center>Tracing and Evaluating OpenAI Agents</center>\n",
        "\n",
        "This guide shows you how to create and evaluate agents with Arize to improve performance. We'll go through the following steps:\n",
        "\n",
        "* Create an agent using the OpenAI agents SDK\n",
        "\n",
        "* Trace the agent activity\n",
        "\n",
        "* Create a dataset to benchmark performance\n",
        "\n",
        "* Run an experiment to evaluate agent performance using LLM as a judge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baTNFxbwX1e2"
      },
      "source": [
        "# Initial setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n69HR7eJswNt"
      },
      "source": [
        "### Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "duKghBmldKsX",
        "outputId": "bae1aebc-0e03-49db-9327-ab81d5c6fff0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.4/114.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.1/239.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.0/812.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q arize-otel openinference-instrumentation-openai-agents openinference-instrumentation-openai arize-phoenix-evals \"arize[Datasets]\"\n",
        "\n",
        "!pip install -q openai opentelemetry-sdk opentelemetry-exporter-otlp gcsfs nest_asyncio openai-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQnyEnJisyn3"
      },
      "source": [
        "### Setup Keys\n",
        "\n",
        "Copy the Arize `API_KEY` and `SPACE_ID` from your Space Settings page (shown below) to the variables in the cell below.\n",
        "\n",
        "<center><img src=\"https://storage.googleapis.com/arize-assets/barcelos/Screenshot%202024-11-11%20at%209.28.27%E2%80%AFPM.png\" width=\"700\"></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMoF2BWgdKsX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nest_asyncio\n",
        "from getpass import getpass\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from google.colab import userdata\n",
        "SPACE_ID = userdata.get('ArizeSpace')\n",
        "API_KEY =  userdata.get('ArizeAPI')\n",
        "OPENAI_API_KEY = userdata.get('openAI')\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfid5cE99yN5"
      },
      "source": [
        "### Setup Tracing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLk_fL7EdKsY"
      },
      "outputs": [],
      "source": [
        "from arize.otel import register\n",
        "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
        "from openinference.instrumentation.openai_agents import OpenAIAgentsInstrumentor\n",
        "\n",
        "# Setup OpenTelemetry via our convenience function\n",
        "tracer_provider = register(\n",
        "    space_id=SPACE_ID,\n",
        "    api_key=API_KEY,\n",
        "    project_name=\"agents-cookbook\",\n",
        ")\n",
        "\n",
        "# Start instrumentation\n",
        "OpenAIAgentsInstrumentor().instrument(tracer_provider=tracer_provider)\n",
        "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLVAqLi5_KAi"
      },
      "source": [
        "# Create your first agent with the OpenAI SDK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUco4xj9dKsY"
      },
      "source": [
        "Here we've setup a basic agent that can solve math problems.\n",
        "\n",
        "We have a function tool that can solve math equations, and an agent that can use this tool.\n",
        "\n",
        "We'll use the `Runner` class to run the agent and get the final output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THL92YjDdKsY"
      },
      "outputs": [],
      "source": [
        "from agents import function_tool, Runner\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def solve_equation(equation: str) -> str:\n",
        "    \"\"\"Use python to evaluate the math equation, instead of thinking about it yourself.\n",
        "\n",
        "    Args:\n",
        "       equation: string which to pass into eval() in python\n",
        "    \"\"\"\n",
        "    return str(eval(equation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciTS8r_5dKsY"
      },
      "outputs": [],
      "source": [
        "from agents import Agent\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Math Solver\",\n",
        "    instructions=\"You solve math problems by evaluating them with python and returning the result\",\n",
        "    tools=[solve_equation],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-ODC3eHdKsY"
      },
      "outputs": [],
      "source": [
        "result = await Runner.run(agent, \"what is 15 + 28?\")\n",
        "\n",
        "# Run Result object\n",
        "print(result)\n",
        "\n",
        "# Get the final output\n",
        "print(result.final_output)\n",
        "\n",
        "# Get the entire list of messages recorded to generate the final output\n",
        "print(result.to_input_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq4rcseCGKRc"
      },
      "source": [
        "Now we have a basic agent, let's evaluate whether the agent responded correctly!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYiOFwpcdKsY"
      },
      "source": [
        "# Evaluating our agent\n",
        "\n",
        "Agents can go awry for a variety of reasons.\n",
        "1. Tool call accuracy - did our agent choose the right tool with the right arguments?\n",
        "2. Tool call results - did the tool respond with the right results?\n",
        "3. Agent goal accuracy - did our agent accomplish the stated goal and get to the right outcome?\n",
        "\n",
        "We'll setup a simple evaluator that will check if the agent's response is correct, you can read about different types of agent evals [here](https://docs.arize.com/arize/llm-evaluation-and-annotations/how-does-evaluation-work/agent-evaluation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPUw0RUYdKsY"
      },
      "source": [
        "Let's setup our evaluation by defining our task function, our evaluator, and our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNVZjL5AdKsY"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from agents import Runner\n",
        "\n",
        "\n",
        "# This is our task function. It takes a question and returns the final output and the messages recorded to generate the final output.\n",
        "async def solve_math_problem(dataset_row: dict):\n",
        "    result = await Runner.run(agent, dataset_row.get(\"question\"))\n",
        "    # OPTIONAL: You don't need to return the messages unless you want to use them in your eval\n",
        "    return {\n",
        "        \"final_output\": result.final_output,\n",
        "        \"messages\": result.to_input_list(),\n",
        "    }\n",
        "\n",
        "\n",
        "dataset_row = {\"question\": \"What is 15 + 28?\"}\n",
        "\n",
        "result = asyncio.run(solve_math_problem(dataset_row))\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj6yUmBKdKsZ"
      },
      "source": [
        "Let's create our evaluator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROroeFiKdKsZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from phoenix.evals import OpenAIModel, llm_classify\n",
        "from arize.experimental.datasets.experiments.types import EvaluationResult\n",
        "\n",
        "\n",
        "def correctness_eval(dataset_row: dict, output: dict) -> EvaluationResult:\n",
        "    # Create a dataframe with the question and answer\n",
        "    df_in = pd.DataFrame(\n",
        "        {\"question\": [dataset_row.get(\"question\")], \"response\": [output]}\n",
        "    )\n",
        "\n",
        "    # Template for evaluating math problem solutions\n",
        "    MATH_EVAL_TEMPLATE = \"\"\"\n",
        "    You are evaluating whether a math problem was solved correctly.\n",
        "\n",
        "    [BEGIN DATA]\n",
        "    ************\n",
        "    [Question]: {question}\n",
        "    ************\n",
        "    [Response]: {response}\n",
        "    [END DATA]\n",
        "\n",
        "    Assess if the answer to the math problem is correct. First work out the correct answer yourself,\n",
        "    then compare with the provided response. Consider that there may be different ways to express the same answer\n",
        "    (e.g., \"43\" vs \"The answer is 43\" or \"5.0\" vs \"5\").\n",
        "\n",
        "    Your answer must be a single word, either \"correct\" or \"incorrect\"\n",
        "    \"\"\"\n",
        "\n",
        "    # Run the evaluation\n",
        "    rails = [\"correct\", \"incorrect\"]\n",
        "    eval_df = llm_classify(\n",
        "        data=df_in,\n",
        "        template=MATH_EVAL_TEMPLATE,\n",
        "        model=OpenAIModel(model=\"gpt-4o\"),\n",
        "        rails=rails,\n",
        "        provide_explanation=True,\n",
        "    )\n",
        "\n",
        "    # Extract results\n",
        "    label = eval_df[\"label\"][0]\n",
        "    score = 1 if label == \"correct\" else 0\n",
        "    explanation = eval_df[\"explanation\"][0]\n",
        "\n",
        "    # Return the evaluation result\n",
        "    return EvaluationResult(score=score, label=label, explanation=explanation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0Qvn8tAs9vL"
      },
      "source": [
        "# Create synthetic dataset of questions\n",
        "\n",
        "Using the template below, we're going to generate a dataframe of 25 questions we can use to test our math problem solving agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05mFQhwbdKsZ"
      },
      "outputs": [],
      "source": [
        "MATH_GEN_TEMPLATE = \"\"\"\n",
        "You are an assistant that generates diverse math problems for testing a math solver agent.\n",
        "The problems should include:\n",
        "\n",
        "Basic Operations: Simple addition, subtraction, multiplication, division problems.\n",
        "Complex Arithmetic: Problems with multiple operations and parentheses following order of operations.\n",
        "Exponents and Roots: Problems involving powers, square roots, and other nth roots.\n",
        "Percentages: Problems involving calculating percentages of numbers or finding percentage changes.\n",
        "Fractions: Problems with addition, subtraction, multiplication, or division of fractions.\n",
        "Algebra: Simple algebraic expressions that can be evaluated with specific values.\n",
        "Sequences: Finding sums, products, or averages of number sequences.\n",
        "Word Problems: Converting word problems into mathematical equations.\n",
        "\n",
        "Do not include any solutions in your generated problems.\n",
        "\n",
        "Respond with a list, one math problem per line. Do not include any numbering at the beginning of each line.\n",
        "Generate 25 diverse math problems. Ensure there are no duplicate problems.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckP5Cm11dKsZ"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "pd.set_option(\"display.max_colwidth\", 500)\n",
        "\n",
        "# Initialize the model\n",
        "model = OpenAIModel(model=\"gpt-4o\", max_tokens=1300)\n",
        "\n",
        "# Generate math problems\n",
        "resp = model(MATH_GEN_TEMPLATE)\n",
        "\n",
        "# Create DataFrame\n",
        "split_response = resp.strip().split(\"\\n\")\n",
        "math_problems_df = pd.DataFrame(split_response, columns=[\"question\"])\n",
        "print(math_problems_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGIbV49kHp4H"
      },
      "source": [
        "Now let's use this dataset and run it with the agent!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHYgS5cpRE3b"
      },
      "source": [
        "# Create an experiment\n",
        "\n",
        "With our dataset of questions we generated above, we can use our experiments feature to track changes across models, prompts, parameters for our agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgTEu7U4Rd5i"
      },
      "source": [
        "Let's create this dataset and upload it into the platform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTFKk77tdKsZ"
      },
      "outputs": [],
      "source": [
        "from arize.experimental.datasets import ArizeDatasetsClient\n",
        "from uuid import uuid1\n",
        "from arize.experimental.datasets.utils.constants import GENERATIVE\n",
        "\n",
        "# Set up the arize client\n",
        "arize_client = ArizeDatasetsClient(api_key=API_KEY)\n",
        "\n",
        "dataset_name = \"math-questions-\" + str(uuid1())[:5]\n",
        "\n",
        "dataset_id = arize_client.create_dataset(\n",
        "    space_id=SPACE_ID,\n",
        "    dataset_name=dataset_name,\n",
        "    dataset_type=GENERATIVE,\n",
        "    data=math_problems_df,\n",
        ")\n",
        "dataset = arize_client.get_dataset(space_id=SPACE_ID, dataset_id=dataset_id)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQHKyCqjdKsZ"
      },
      "outputs": [],
      "source": [
        "experiment_id, experiment_dataframe = arize_client.run_experiment(\n",
        "    space_id=SPACE_ID,\n",
        "    dataset_id=dataset_id,\n",
        "    task=solve_math_problem,\n",
        "    evaluators=[correctness_eval],\n",
        "    experiment_name=f\"solve-math-questions-{str(uuid1())[:5]}\",\n",
        "    dry_run=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCudJtk3dKsZ"
      },
      "outputs": [],
      "source": [
        "experiment_dataframe"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}